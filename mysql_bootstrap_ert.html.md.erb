---
title: Recovering MySQL from Elastic Runtime Downtime
owner: RelEng
---

<strong><%= modified_date %></strong>

_This topic assumes you are using [BOSH CLI v2](https://bosh.io/docs/cli-v2.html)._

This topic describes the procedure for recovering a terminated Elastic Runtime cluster using a process known as bootstrapping.

<p class="note"><strong>Note</strong>: By default, <code>mysql</code> clients are not allowed to connect as administrator to internal MySQL instances from remote hosts. Administrators must <code>bosh ssh</code> into each internal MySQL VM to connect as the MySQL super user. This affects the admin and roadmin accounts.</p>

## <a id='when'></a>When to Bootstrap

You must bootstrap a cluster that loses quorum. A cluster loses quorum when less than half of the nodes can communicate with each other for longer than the configured grace period. If a cluster does not lose quorum, individual unhealthy nodes automatically rejoin the cluster after resolving the error, restarting the node, or restoring connectivity. 

You can detect lost quorum through the following symptoms:

* All nodes appear "Unhealthy" on the proxy dashboard, viewable at `proxy-BOSH-JOB-INDEX.p-mysql.YOUR-SYSTEM-DOMAIN`:
  <img src="https://docs.pivotal.io/p-mysql/1-8/quorum-lost.png">
* All responsive nodes report the value of `wsrep_cluster_status` as `non-Primary`:

    <pre class="terminal">
    mysql> SHOW STATUS LIKE 'wsrep_cluster_status';
    +----------------------+-------------+
    | Variable_name        | Value       |
    +----------------------+-------------+
    | wsrep_cluster_status | non-Primary |
    +----------------------+-------------+
    </pre>

* All responsive nodes respond with `ERROR 1047` when queried with most statement types:

    <pre class="terminal">
    mysql> select * from mysql.user;
    ERROR 1047 (08S01) at line 1: WSREP has not yet prepared node for application use
    </pre>

See the [Cluster Scaling, Node Failure, and Quorum](http://docs.pivotal.io/p-mysql/1-8/cluster-behavior.html) topic for more details about determining cluster state.

Follow the steps below to recover a cluster that has lost quorum.

## <a id='manifest'></a>Step 1: Choose the Correct Manifest

1. Log in to the BOSH Director by running `bosh2 -e MY-ENV log-in`. Replace `MY-ENV` with the environment where you deployed the cluster.
    <pre class="terminal">
    $ bosh2 -e prod log-in
    </pre>

1. Run `bosh2 -e MY-ENV deployments`. Replace `MY-ENV` with the environment where you deployed the cluster.

    <pre class="terminal">
    $ bosh2 -e prod deployments
    Using environment '192.168.56.6' as client 'admin'
    Name                Release(s)                Stemcell(s)                                         Team(s)  Cloud Config
    cf                  binary-buildpack/1.0.9    bosh-warden-boshlite-ubuntu-trusty-go<\_agent/3363.9  -        latest
                        capi/1.21.0
                        cf-mysql/34
                        cf-smoke-tests/11
                        cflinuxfs2-rootfs/1.52.0
                        consul/155
                        diego/1.8.1
                        garden-runc/1.2.0
                        loggregator/78
                        nats/15
                        routing/0.145.0
                        statsd-injector/1.0.20
                        uaa/25
    service-instance    mysql/0.6.0               bosh-warden-boshlite-ubuntu-trusty-go\_agent/3363.9  -        latest

    2 deployments

    Succeeded
    </pre>

1.  Run `bosh2 -e MY-ENV -d MY-DEPLOYMENT manifest > /tmp/MANIFEST.yml` to download the manifest. Replace the example text with the following:
  * `MY-ENV`: the environment where you deployed the cluster
  * `MY-DEPLOYMENT`: the name of your deployment cluster
  * `MANIFEST.yml`: the name you want to give the manifest

    <pre class="terminal">
    $ bosh2 -e prod -d mysql manifest /tmp/mysql.yml
    </pre>

## <a id="assisted-bootstrap"></a>Step 2: Run the Bootstrap Errand

Elastic Runtime versions 1.7.0 and later include a [BOSH errand](http://bosh.io/docs/jobs.html#jobs-vs-errands) to automate the process of bootstrapping. The bootstrap errand automates the steps described in the [Manual Bootstrapping](#manual-bootstrap) section below. It finds the node with the highest transaction sequence number and asks it to start up by itself in bootstrap mode. Finally, it asks the remaining nodes to join the cluster.

In most cases, running the errand will recover your cluster. However, certain scenarios require additional steps. To determine which set of instructions to follow, you must determine the state of your Virtual Machines (VMs).

1. Run `bosh2 -e MY-ENV -d MY-DEPLOYMENT instances` and examine the output. Replace `MY-ENV` with the environment where you deployed the cluster and `MY-DEPLOYMENT` with the deployment cluster name.
    <pre class="terminal">
    $ bosh2 -e prod -d mysql instances
    </pre>
    * If the output of `bosh2 instances` shows the state of the jobs as `failing`, proceed to Scenario 1.
    * If the output of `bosh2 instances` shows the state of jobs as `unknown/unknown`, proceed to Scenario 2.

### <a id="cluster-disrupted"></a>Scenario 1: Virtual Machines Running, Cluster Disrupted

In this scenario, nodes are up and running, but the cluster has been disrupted. You can run the bootstrap errand without recreating the VMs.

1. Run `bosh2 -e MY-ENV -d MY-DEPLOYMENT run-errand bootstrap`. Replace `MY-ENV` with the name of the environment where you deployed the cluster and `MY-DEPLOYMENT` with the deployment cluster name.
    <pre class="terminal">
    $ bosh2 -e prod -d mysql run-errand bootstrap
    </pre>
    <p class="note"><strong>Note</strong>: Sometimes the bootstrap errand fails on the first try. If this happens, run the command again in a few minutes.</p>

1. If the errand fails, try performing the steps automated by the errand manually by following the [Manual Bootstrapping](#manual-bootstrap) procedure.

### <a id="vms-terminated"></a>Scenario 2: Virtual Machines Terminated or Lost

In this scenario, severe circumstances such as power failure have terminated all of your VMs. You need to recreate the VMs before you can recover the cluster.

1. If you enabled the [VM Resurrector](./resurrector.html#enabling) in Ops Manager, the system detects the terminated VMs and automatically attempts to recreate them. Run `bosh2 -e MY-ENV tasks` to see the `scan and fix` job run by the VM Resurrector.

    <pre class="terminal">
    $ bosh2 -e prod tasks
    </pre>

    If you have not enabled the VM Resurrector, run the BOSH Cloudcheck command `bosh2 -e MY-ENV -d MY-DEPLOYMENT cloud-check` to delete any placeholder VMs. When prompted, choose `Delete VM reference` by entering `6`.

    <pre class="terminal">
    $ bosh2 -e prod -d mysql cloud-check

    Using environment '192.168.56.6' as user 'director' (bosh.*.read, openid, bosh.*.admin, bosh.read, bosh.admin)

    Task 34

    19:19:12 | Scanning 21 VMs: Checking VM states (00:00:16)
    19:19:28 | Scanning 21 VMs: 19 OK, 2 unresponsive, 0 missing, 0 unbound (00:00:00)
    19:19:28 | Scanning 5 persistent disks: Looking for inactive disks (00:00:00)
    19:19:28 | Scanning 5 persistent disks: 5 OK, 0 missing, 0 inactive, 0 mount-info mismatch (00:00:00)

    Started  Fri Aug  4 19:19:12 UTC 2017
    Finished Fri Aug  4 19:19:28 UTC 2017
    Duration 00:00:16

    Task 34 done

    #  Type                Description
    1  unresponsive_agent  VM for 'uaa/0 (0)' with cloud ID 'vm-001' is not responding.
    2  unresponsive_agent  VM for 'mysql/0 (0)' with cloud ID 'vm-007' is not responding.

    2 problems

    1: Skip for now
    2: Reboot VM
    3: Recreate VM without waiting for processes to start
    4: Recreate VM and wait for processes to start
    5: Delete VM
    6: Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)
    </pre>

1. Run `bosh2 -e MY-ENV -d MY-DEPLOYMENT instances` and examine the output.
Do not proceed to the next step until all VMs are in either the `starting` or `failing` state.

    <pre class="terminal">
    $ bosh2 -e prod -d mysql instances
    </pre>

1. Complete the following steps to prepare your deployment for the bootstrap errand:

    1. Open `/tmp/MANIFEST.yml` in a text editor.
    1. Search for the jobs section: `jobs`.
    1. Search for the mysql-partition: `mysql-partition`.
    1. Search for the update section: `update`.
    1. Change `max_in_flight` to `3`.
    1. Below the `max_in_flight` line, add a new line: `canaries: 0`.
    1. Set `update.serial` to `false`.
    1. Run `bosh2 -e MY-ENV -d MY-DEPLOYMENT deploy /tmp/MANIFEST.yml`.

1. Run `bosh2 -e MY-ENV -d MY-DEPLOYMENT run-errand bootstrap`. Replace `MY-ENV` with the name of the environment where you deployed the cluster and `MY-DEPLOYMENT` with the deployment cluster name.

1. Run `bosh2 -e MY-ENV -d MY-DEPLOYMENT instances` and examine the output to confirm that the errand completes successfully. Some instances may still appear as `failing`. 

1. Complete the following steps to restore the BOSH configuration:

    1. Open `/tmp/MANIFEST.yml` in a text editor.
    1. Re-set `canaries` to 1, `max_in_flight` to 1, and `serial` to true in the same manner as above.
    1. Run `bosh2 -e MY-ENV -d MY-DEPLOYMENT deploy /tmp/MANIFEST.yml`.
    1. Validate that all mysql instances are in `running` state.

    <p class="note"><strong>Note:</strong> You must reset the values in the BOSH manifest to ensure successful future deployments and accurate reporting of the status of your jobs.</p>

1. If this procedure fails, try performing the steps automated by the errand manually by following the [Manual Bootstrapping](#manual-bootstrap) procedure.

## <a id="manual-bootstrap"></a>Manual Bootstrapping ##

<p class="note"><strong>Note</strong>: The following steps are prone to user error and can result in lost data if followed incorrectly. Please follow the <a href="#assisted-bootstrap">Run the Bootstrap Errand</a> instructions above first, and only resort to the manual process if the errand fails to repair the cluster.</p>
If the bootstrap errand cannot recover the cluster, you need to perform the steps automated by the errand manually. 

- If the output of `bosh2 instances` shows the state of the jobs as `failing` ([Scenario 1](#cluster-disrupted)), proceed directly to the manual steps below.
- If the output of `bosh2 instances` shows the state of the jobs as `unknown/unknown`, perform Steps 1-3 of [Scenario 2](#vms-terminated), substitute the manual steps below for Step 4, and then perform Steps 5-6 of [Scenario 2](#vms-terminated).

1. SSH to each node in the cluster and, as root, shut down the `mariadb` process.

    <pre class="terminal">
    $ monit stop mariadb_ctrl
    </pre>

    Re-bootstrapping the cluster will not be successful unless all other nodes have been shut down.

1. Choose a node to bootstrap by locating the node with the highest transaction sequence number (`seqno`). You can obtain the `seqno` of a stopped node in one of two ways:
    - If a node shut down gracefully, the `seqno` is in the Galera state file of the node.
    <pre class="terminal">
    $ cat /var/vcap/store/mysql/grastate.dat | grep 'seqno:'
    </pre>
    - If the node crashed or was killed, the `seqno` in the Galera state file of the node is `-1`. In this case, the `seqno` may be recoverable from the database. 
        1. Run the following command to start up the database, log the recovered sequence number, and exit.
        <pre class="terminal">
        $ /var/vcap/packages/mariadb/bin/mysqld --wsrep-recover
        </pre>
        1. Scan the error log for the recovered sequence number. The last number after the group id (`uuid`) is the recovered `seqno`:
        <pre class="terminal">
        $ grep "Recovered position" /var/vcap/sys/log/mysql/mysql.err.log | tail -1
        150225 18:09:42 mysqld_safe WSREP: Recovered position e93955c7-b797-11e4-9faa-9a6f0b73eb46:15
        </pre>
        If the node never connected to the cluster before crashing, it may not have a group id (`uuid` in `grastate.dat`). In this case, you cannot recover the `seqno`. Unless all nodes crashed this way, do not choose this node for bootstrapping.
    
1. Choose the node with the highest `seqno` value as the bootstrap node. If all nodes have the same `seqno`, you can choose any node as the bootstrap node.

    <p class="note"><strong>Note:</strong> Only perform these bootstrap commands on the node with the highest <code>seqno</code>. Otherwise, the node with the highest <code>seqno</code> will be unable to join the new cluster unless its data is abandoned. Its <code>mariadb</code> process will exit with an error. See the <a href="http://docs.pivotal.io/p-mysql/1-8/cluster-behavior.html">Cluster Scaling, Node Failure, and Quorum</a> topic for more details about intentionally abandoning data.</p>

1. On the bootstrap node, update the state file and restart the `mariadb` process.

    <pre class="terminal">
    $ echo -n "NEEDS_BOOTSTRAP" > /var/vcap/store/mysql/state.txt
    $ monit start mariadb_ctrl
    </pre>

1. Check that the `mariadb` process has started successfully.

    <pre class="terminal">
    $ watch monit summary
    </pre>

    It can take up to ten minutes for `monit` to start the `mariadb` process.
    
1. Once the bootstrapped node is running, start the `mariadb` process on the remaining nodes using `monit`.

    <pre class="terminal">
    $ monit start mariadb_ctrl
    </pre>

1. Verify that the new nodes have successfully joined the cluster. The following command displays the total number of nodes in the cluster:

    <pre class="terminal">
    mysql> SHOW STATUS LIKE 'wsrep_cluster_size';
    </pre>

1. Complete the following steps to restore the BOSH configuration:

    1. Open `/tmp/MANIFEST.yml` in a text editor.
    1. Re-set `canaries` to 1, `max_in_flight` to 1, and `serial` to true in the same manner as above.
    1. Run `bosh2 -e MY-ENV -d MY-DEPLOYMENT deploy /tmp/MANIFEST.yml`.
    1. Validate that all mysql instances are in `running` state.

    <p class="note"><strong>Note:</strong> You must reset the values in the BOSH manifest to ensure successful future deployments and accurate reporting of the status of your jobs.</p>